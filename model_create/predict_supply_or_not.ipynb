{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import pickle\n",
    "import csv\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jinhyeok = pd.read_csv(\"./data/weather_area_op.csv\")\n",
    "df_jinhyeok.rename(columns={\"Unnamed: 0\":\"행정구역\",\"Unnamed: 1\":\"계절\",\"Unnamed: 2\":\"연도\"},inplace=True)\n",
    "\n",
    "month_spring = [\"_3\",\"_4\",\"_5\"]           # [\"_3\",\"_4\",\"_5\"]\n",
    "month_summer = [\"_6\",\"_7\",\"_8\"]      # [\"_5\",\"_6\",\"_7\",\"_8\"]\n",
    "month_fall = [\"_8\",\"_9\",\"_10\"]      # [\"_8\",\"_9\",\"_10\",\"_11\"]\n",
    "month_winter = [\"_9\",\"_10\",\"_11\"]      # [\"_8\",\"_9\",\"_10\",\"_11\",\"_12\",\"_1\",\"_2\"]\n",
    "\n",
    "season = [\"spring\", \"summer\", \"fall\", \"winter\"]\n",
    "\n",
    "for i in range(len(season)) : \n",
    "    globals()[\"df_\"+season[i]] = df_jinhyeok[df_jinhyeok[\"계절\"]==season[i]]  # 계절별로 데이터 프레임 형성\n",
    "    globals()[\"df_\"+season[i]] = globals()[\"df_\"+season[i]].groupby([\"행정구역\",\"계절\",\"연도\"]).mean()   # 행정구역, 계절, 연도를 인덱스로 groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/weather_area_op.csv\", \"r\", encoding=\"utf8\") as f: \n",
    "    dr = csv.reader(f)\n",
    "    s = pd.DataFrame(dr)\n",
    "    s = s.rename(columns=s.iloc[0])\n",
    "    s.drop(index=s.index[0], axis=0,inplace=True)\n",
    "\n",
    "    s.columns = [\"지역\", '계절', '연도', '평균기온(°C)_1월', '최고기온(°C)_1월', '최저기온(°C)_1월',\n",
    "       '월합강수량(00~24h만)(mm)_1월', '합계 일사량(MJ/m2)_1월', '평균기온(°C)_2월',\n",
    "       '최고기온(°C)_2월', '최저기온(°C)_2월', '월합강수량(00~24h만)(mm)_2월',\n",
    "       '합계 일사량(MJ/m2)_2월', '평균기온(°C)_3월', '최고기온(°C)_3월', '최저기온(°C)_3월',\n",
    "       '월합강수량(00~24h만)(mm)_3월', '합계 일사량(MJ/m2)_3월', '평균기온(°C)_4월',\n",
    "       '최고기온(°C)_4월', '최저기온(°C)_4월', '월합강수량(00~24h만)(mm)_4월',\n",
    "       '합계 일사량(MJ/m2)_4월', '평균기온(°C)_5월', '최고기온(°C)_5월', '최저기온(°C)_5월',\n",
    "       '월합강수량(00~24h만)(mm)_5월', '합계 일사량(MJ/m2)_5월', '평균기온(°C)_6월',\n",
    "       '최고기온(°C)_6월', '최저기온(°C)_6월', '월합강수량(00~24h만)(mm)_6월',\n",
    "       '합계 일사량(MJ/m2)_6월', '평균기온(°C)_7월', '최고기온(°C)_7월', '최저기온(°C)_7월',\n",
    "       '월합강수량(00~24h만)(mm)_7월', '합계 일사량(MJ/m2)_7월', '평균기온(°C)_8월',\n",
    "       '최고기온(°C)_8월', '최저기온(°C)_8월', '월합강수량(00~24h만)(mm)_8월',\n",
    "       '합계 일사량(MJ/m2)_8월', '평균기온(°C)_9월', '최고기온(°C)_9월', '최저기온(°C)_9월',\n",
    "       '월합강수량(00~24h만)(mm)_9월', '합계 일사량(MJ/m2)_9월', '평균기온(°C)_10월',\n",
    "       '최고기온(°C)_10월', '최저기온(°C)_10월', '월합강수량(00~24h만)(mm)_10월',\n",
    "       '합계 일사량(MJ/m2)_10월', '평균기온(°C)_11월', '최고기온(°C)_11월', '최저기온(°C)_11월',\n",
    "       '월합강수량(00~24h만)(mm)_11월', '합계 일사량(MJ/m2)_11월', '평균기온(°C)_12월',\n",
    "       '최고기온(°C)_12월', '최저기온(°C)_12월', '월합강수량(00~24h만)(mm)_12월',\n",
    "       '합계 일사량(MJ/m2)_12월', '면적 (ha)', '생산량 (톤)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = \"|\".join(w for w in month_spring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(season)) : \n",
    "    globals()[\"month_str_\"+season[i]] = \"|\".join(w for w in globals()[\"month_\"+season[i]])  # 각 계절별 월을 | 으로 join\n",
    "    \n",
    "    # 각 계절이 속하는 월과 면적, 생산이 속하는 컬럼명을 리스트로 저장\n",
    "    globals()[\"col_of_\"+season[i]] = globals()[\"df_\"+season[i]].columns[globals()[\"df_\"+season[i]].columns.str.contains(globals()[\"month_str_\"+season[i]]+\"|면적|생산\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임을 계절별로 분리\n",
    "df_spring = df_spring[col_of_spring]\n",
    "df_summer = df_summer[col_of_summer]\n",
    "df_fall = df_fall[col_of_fall]\n",
    "df_winter = df_winter[col_of_winter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_winter.dropna(how=\"any\",inplace=True)\n",
    "# df_winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = [\"서울특별시\", \"부산광역시\", \"대구광역시\", \"인천광역시\", \"광주광역시\", \"대전광역시\", \"울산광역시\", \"세종특별자치시\", \"경기도\", \"강원도\", \"충청북도\", \"충청남도\", \"전라북도\", \"전라남도\", \"경상북도\", \"경상남도\", \"제주도\"]\n",
    "city = sorted(city)\n",
    "# city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_winter[df_winter[\"생산량 (톤)\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생산량이 NaN인 행을 drop\n",
    "df_spring.dropna(how=\"any\", inplace=True)\n",
    "df_summer.dropna(how=\"any\", inplace=True)\n",
    "df_fall.dropna(how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 면적 컬럼 삭제\n",
    "for s in season : \n",
    "    globals()[\"df_\"+s].drop(columns=\"면적 (ha)\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배추 생산 시기를 임의로 상, 중, 하로 나누었으며, 거기에는 육묘기, 생육기, 결구기 등이 포함돼 있음\n",
    "for s in season : \n",
    "    globals()[\"df_\"+s].columns = ['평균기온(°C)_상', '최고기온(°C)_상', '최저기온(°C)_상', '월합강수량(00~24h만)(mm)_상',\n",
    "       '합계 일사량(MJ/m2)_상', '평균기온(°C)_중', '최고기온(°C)_중', '최저기온(°C)_중',\n",
    "       '월합강수량(00~24h만)(mm)_중', '합계 일사량(MJ/m2)_중', '평균기온(°C)_하',\n",
    "       '최고기온(°C)_하', '최저기온(°C)_하', '월합강수량(00~24h만)(mm)_하',\n",
    "       '합계 일사량(MJ/m2)_하', '생산량 (톤)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>평균기온(°C)_상</th>\n",
       "      <th>최고기온(°C)_상</th>\n",
       "      <th>최저기온(°C)_상</th>\n",
       "      <th>월합강수량(00~24h만)(mm)_상</th>\n",
       "      <th>합계 일사량(MJ/m2)_상</th>\n",
       "      <th>평균기온(°C)_중</th>\n",
       "      <th>최고기온(°C)_중</th>\n",
       "      <th>최저기온(°C)_중</th>\n",
       "      <th>월합강수량(00~24h만)(mm)_중</th>\n",
       "      <th>합계 일사량(MJ/m2)_중</th>\n",
       "      <th>평균기온(°C)_하</th>\n",
       "      <th>최고기온(°C)_하</th>\n",
       "      <th>최저기온(°C)_하</th>\n",
       "      <th>월합강수량(00~24h만)(mm)_하</th>\n",
       "      <th>합계 일사량(MJ/m2)_하</th>\n",
       "      <th>생산량 (톤)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>행정구역</th>\n",
       "      <th>계절</th>\n",
       "      <th>연도</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">강원도</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">spring</th>\n",
       "      <th>2000</th>\n",
       "      <td>4.40</td>\n",
       "      <td>19.30</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>14.10</td>\n",
       "      <td>448.65</td>\n",
       "      <td>10.40</td>\n",
       "      <td>23.7</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>37.10</td>\n",
       "      <td>486.355</td>\n",
       "      <td>15.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>3.90</td>\n",
       "      <td>75.20</td>\n",
       "      <td>520.280</td>\n",
       "      <td>87619.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>5.25</td>\n",
       "      <td>19.75</td>\n",
       "      <td>-8.6</td>\n",
       "      <td>2.55</td>\n",
       "      <td>455.28</td>\n",
       "      <td>10.85</td>\n",
       "      <td>23.7</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>23.55</td>\n",
       "      <td>530.950</td>\n",
       "      <td>17.05</td>\n",
       "      <td>31.4</td>\n",
       "      <td>5.55</td>\n",
       "      <td>67.55</td>\n",
       "      <td>562.770</td>\n",
       "      <td>83405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>7.30</td>\n",
       "      <td>19.70</td>\n",
       "      <td>-7.3</td>\n",
       "      <td>33.10</td>\n",
       "      <td>519.21</td>\n",
       "      <td>12.70</td>\n",
       "      <td>24.8</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>50.00</td>\n",
       "      <td>542.480</td>\n",
       "      <td>17.70</td>\n",
       "      <td>34.1</td>\n",
       "      <td>5.70</td>\n",
       "      <td>65.00</td>\n",
       "      <td>586.320</td>\n",
       "      <td>56908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>6.60</td>\n",
       "      <td>21.30</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>28.50</td>\n",
       "      <td>523.00</td>\n",
       "      <td>11.90</td>\n",
       "      <td>25.9</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>49.00</td>\n",
       "      <td>540.765</td>\n",
       "      <td>17.50</td>\n",
       "      <td>34.5</td>\n",
       "      <td>5.60</td>\n",
       "      <td>52.60</td>\n",
       "      <td>586.195</td>\n",
       "      <td>82760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>6.90</td>\n",
       "      <td>20.30</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>19.90</td>\n",
       "      <td>481.75</td>\n",
       "      <td>12.60</td>\n",
       "      <td>25.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>16.30</td>\n",
       "      <td>537.210</td>\n",
       "      <td>17.80</td>\n",
       "      <td>33.9</td>\n",
       "      <td>5.80</td>\n",
       "      <td>47.70</td>\n",
       "      <td>540.490</td>\n",
       "      <td>100486.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  평균기온(°C)_상  최고기온(°C)_상  최저기온(°C)_상  월합강수량(00~24h만)(mm)_상  \\\n",
       "행정구역 계절     연도                                                               \n",
       "강원도  spring 2000        4.40       19.30        -8.6                 14.10   \n",
       "            2001        5.25       19.75        -8.6                  2.55   \n",
       "            2002        7.30       19.70        -7.3                 33.10   \n",
       "            2003        6.60       21.30        -6.0                 28.50   \n",
       "            2004        6.90       20.30        -4.9                 19.90   \n",
       "\n",
       "                  합계 일사량(MJ/m2)_상  평균기온(°C)_중  최고기온(°C)_중  최저기온(°C)_중  \\\n",
       "행정구역 계절     연도                                                          \n",
       "강원도  spring 2000           448.65       10.40        23.7        -3.1   \n",
       "            2001           455.28       10.85        23.7        -2.2   \n",
       "            2002           519.21       12.70        24.8        -1.2   \n",
       "            2003           523.00       11.90        25.9        -1.4   \n",
       "            2004           481.75       12.60        25.4        -0.5   \n",
       "\n",
       "                  월합강수량(00~24h만)(mm)_중  합계 일사량(MJ/m2)_중  평균기온(°C)_하  \\\n",
       "행정구역 계절     연도                                                        \n",
       "강원도  spring 2000                 37.10          486.355       15.90   \n",
       "            2001                 23.55          530.950       17.05   \n",
       "            2002                 50.00          542.480       17.70   \n",
       "            2003                 49.00          540.765       17.50   \n",
       "            2004                 16.30          537.210       17.80   \n",
       "\n",
       "                  최고기온(°C)_하  최저기온(°C)_하  월합강수량(00~24h만)(mm)_하  \\\n",
       "행정구역 계절     연도                                                   \n",
       "강원도  spring 2000        31.6        3.90                 75.20   \n",
       "            2001        31.4        5.55                 67.55   \n",
       "            2002        34.1        5.70                 65.00   \n",
       "            2003        34.5        5.60                 52.60   \n",
       "            2004        33.9        5.80                 47.70   \n",
       "\n",
       "                  합계 일사량(MJ/m2)_하   생산량 (톤)  \n",
       "행정구역 계절     연도                               \n",
       "강원도  spring 2000          520.280   87619.0  \n",
       "            2001          562.770   83405.0  \n",
       "            2002          586.320   56908.0  \n",
       "            2003          586.195   82760.0  \n",
       "            2004          540.490  100486.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total = pd.concat([df_spring, df_summer, df_fall, df_winter])\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total[df_total[\"생산량 (톤)\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생산량이 0이상인 지역의 기후 조건을 적합, 0인 지역의 기후 조건을 부적합하다고 labeling\n",
    "df_total[\"재배 적합\"] = df_total[\"생산량 (톤)\"].apply(lambda x : 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total[df_total[\"재배 적합\"] == 1].tail(30)\n",
    "# df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spring[df_spring[\"생산량 (톤)\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1212 entries, ('강원도', 'spring', 2000) to ('충청북도', 'winter', 2021)\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   평균기온(°C)_상            1212 non-null   float64\n",
      " 1   최고기온(°C)_상            1212 non-null   float64\n",
      " 2   최저기온(°C)_상            1212 non-null   float64\n",
      " 3   월합강수량(00~24h만)(mm)_상  1212 non-null   float64\n",
      " 4   합계 일사량(MJ/m2)_상       1212 non-null   float64\n",
      " 5   평균기온(°C)_중            1212 non-null   float64\n",
      " 6   최고기온(°C)_중            1212 non-null   float64\n",
      " 7   최저기온(°C)_중            1212 non-null   float64\n",
      " 8   월합강수량(00~24h만)(mm)_중  1212 non-null   float64\n",
      " 9   합계 일사량(MJ/m2)_중       1212 non-null   float64\n",
      " 10  평균기온(°C)_하            1212 non-null   float64\n",
      " 11  최고기온(°C)_하            1212 non-null   float64\n",
      " 12  최저기온(°C)_하            1212 non-null   float64\n",
      " 13  월합강수량(00~24h만)(mm)_하  1212 non-null   float64\n",
      " 14  합계 일사량(MJ/m2)_하       1212 non-null   float64\n",
      " 15  재배 적합                 1212 non-null   int64  \n",
      "dtypes: float64(15), int64(1)\n",
      "memory usage: 156.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_total.drop(columns=\"생산량 (톤)\", inplace=True)\n",
    "df_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관 분석(안 해도 되나)\n",
    "# plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "# corr = np.corrcoef(df_total.values.T)\n",
    "# fig = plt.figure(figsize=(16,16))\n",
    "# sns.heatmap(corr, cbar=True, annot=True, square=False, xticklabels=df_total.columns, yticklabels=df_total.columns, annot_kws={\"size\":7})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, test set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_total.iloc[:,:-1]\n",
    "y = df_total.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "std.fit(X)\n",
    "X = std.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmsc = MinMaxScaler()\n",
    "mmsc.fit(X)\n",
    "X = std.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV를 통한 최적의 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증을 위한 kfold\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=27)\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=27)\n",
    "\n",
    "alpha = np.logspace(-4,2,7)   # lasso, ridge 해당\n",
    "learning_rate = np.logspace(-4,2,7)   # adaboost, gradientboost 해당\n",
    "eta = np.logspace(-4,2,7)   # xgboost 해당\n",
    "max_iter = [1,3,5,10,20,50,100,150,200]   # lasso, ridge 해당\n",
    "solver = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']   # ridge 해당, 계산에 사용할 알고리즘\n",
    "n_estimators = [1,3,5,10,15,20,50,100,200,500]   # randomforesteregressor(사용할 tree의 개수), gradientboost, adaboost, xgboost 해당\n",
    "max_depth = [2,3,5,7,10,12]  # randomforesteregressor(tree의 깊이), gradientboost, xgboost 해당\n",
    "criterion_rf = [\"gini\", \"entropy\", \"log_loss\"]  # randomforesteregressor 해당\n",
    "criterion_gb = [\"squared_error\", \"friedman_mse\"]  # gradientboost 해당\n",
    "algorithm_ada = [\"SAMME\", \"SAMME.R\"]    # adaboost 해당\n",
    "loss_gb = [\"log_loss\", \"deviance\", \"exponential\"]  # gradientboost 해당\n",
    "gamma = [0, 0.01, 0.05, 0.1, 0.2, 0.5]    # xgboost 해당\n",
    "max_leaf_nodes = [2,3,5,7]     # randomforest, xgboost 해당\n",
    "min_child_weight = [1,2,3,4,5]   # xgboost 해당\n",
    "min_samples_leaf = [1,2,3,4,5]   # randomforesst 해당\n",
    "min_weight_fraction_leaf = [0, 0.1, 0.2, 0.4, 0.5]  # randomforest 해당\n",
    "max_features = [\"sqrt\", \"log2\"]   # randomforest 해당\n",
    "max_features_grad = [\"auto\", \"sqrt\", \"log2\"]   # randomforest 해당\n",
    "\n",
    "params_rf = {\"n_estimators\" : n_estimators, \n",
    "                \"max_depth\" : max_depth, \n",
    "                \"criterion\" : criterion_rf,\n",
    "                \"max_leaf_nodes\" : max_leaf_nodes,\n",
    "                \"min_samples_leaf\" : min_samples_leaf,\n",
    "                \"min_weight_fraction_leaf\" : min_weight_fraction_leaf,\n",
    "                \"max_features\" : max_features}\n",
    "\n",
    "params_ada = {\"learning_rate\" : learning_rate, \n",
    "                \"n_estimators\" : n_estimators, \n",
    "                \"algorithm\" : algorithm_ada\n",
    "                }\n",
    "\n",
    "params_grad = {\"learning_rate\" : learning_rate, \n",
    "                \"n_estimators\" : n_estimators, \n",
    "                \"max_depth\" : max_depth,\n",
    "                \"criterion\" : criterion_gb,\n",
    "                \"loss\" : loss_gb,\n",
    "                \"max_leaf_nodes\" : max_leaf_nodes,\n",
    "                \"min_samples_leaf\" : min_samples_leaf,\n",
    "                \"min_weight_fraction_leaf\" : min_weight_fraction_leaf,\n",
    "                \"max_features\" : max_features_grad}\n",
    "\n",
    "params_xgb = {\"learning_rate\" : eta,\n",
    "                \"n_estimators\" : n_estimators, \n",
    "                \"max_depth\" : max_depth, \n",
    "                \"gamma\" : gamma,\n",
    "                \"max_leaf_nodes\" : max_leaf_nodes,\n",
    "                \"min_child_weight\" : min_child_weight}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m rf \u001b[39m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m      2\u001b[0m grid_rf \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mrf, param_grid\u001b[39m=\u001b[39mparams_rf, cv\u001b[39m=\u001b[39mskfold, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m grid_rf\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Score :\u001b[39m\u001b[39m\"\u001b[39m, grid_rf\u001b[39m.\u001b[39mbest_score_)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Parameters :\u001b[39m\u001b[39m\"\u001b[39m, grid_rf\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\jhahn\\anaconda3\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "grid_rf = GridSearchCV(estimator=rf, param_grid=params_rf, cv=skfold, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "grid_rf.fit(X, y)\n",
    "print(\"Best Score :\", grid_rf.best_score_)\n",
    "print(\"Best Parameters :\", grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : -0.11474055006096735\n",
      "Best Parameters : {'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "grid_ada = GridSearchCV(estimator=ada, param_grid=params_ada, cv=skfold, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "grid_ada.fit(X, y)\n",
    "print(\"Best Score :\", grid_ada.best_score_)\n",
    "print(\"Best Parameters :\", grid_ada.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : -0.08828749491938762\n",
      "Best Parameters : {'criterion': 'squared_error', 'learning_rate': 1.0, 'loss': 'exponential', 'max_depth': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "grad = GradientBoostingClassifier()\n",
    "grid_grad = GridSearchCV(estimator=grad, param_grid=params_grad, cv=skfold, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "grid_grad.fit(X, y)\n",
    "print(\"Best Score :\", grid_grad.best_score_)\n",
    "print(\"Best Parameters :\", grid_grad.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : -0.0924129521745021\n",
      "Best Parameters : {'eta': 0.1, 'max_depth': 5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "grid_xgb = GridSearchCV(estimator=xgb, param_grid=params_xgb, cv=skfold, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "grid_xgb.fit(X, y)\n",
    "print(\"Best Score :\", grid_xgb.best_score_)\n",
    "print(\"Best Parameters :\", grid_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Best Parameters : {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'max_leaf_nodes': 2, 'min_child_weight': 2, 'n_estimators': 200}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost를 이용한 배추 재배 적합 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=27)\n",
    "skfold2.get_n_splits(X,y)\n",
    "\n",
    "for train_index, test_index in skfold2.split(X, y) : \n",
    "    print(\"Train :\", train_index, \"Test :\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.values.reshape(-1,1)[train_index], y.values.reshape(-1,1)[test_index]\n",
    "\n",
    "xgb2 = XGBClassifier(learning_rate=0.05, gamma=0, max_depth=4, min_child_weight=2, n_estimators=200)\n",
    "xgb2.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xgb2.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : [   0    1    2    3    5    6    7    8    9   10   12   13   14   15\n",
      "   16   17   20   21   22   23   25   26   27   30   31   35   36   37\n",
      "   39   40   41   42   43   44   45   46   47   48   50   52   53   54\n",
      "   56   57   58   60   63   64   65   66   67   68   69   70   71   72\n",
      "   73   74   75   76   77   78   79   80   81   82   84   85   86   87\n",
      "   88   89   90   91   92   93   94   95   96   97   98   99  100  101\n",
      "  103  104  105  106  108  109  111  112  115  117  118  119  120  121\n",
      "  123  124  126  127  129  130  131  132  133  134  136  137  138  140\n",
      "  141  142  143  144  145  148  149  150  151  152  154  157  158  159\n",
      "  160  161  162  163  164  166  167  170  171  172  174  175  177  178\n",
      "  179  180  181  182  183  184  185  186  187  188  189  190  191  192\n",
      "  193  194  195  196  197  198  200  201  202  203  204  205  206  207\n",
      "  210  211  212  213  215  217  218  219  220  221  222  224  226  227\n",
      "  228  229  232  233  234  236  237  238  239  240  241  242  243  244\n",
      "  245  246  247  250  251  252  253  255  256  257  258  259  260  261\n",
      "  262  263  265  266  267  268  269  270  271  272  273  274  275  277\n",
      "  278  279  280  281  282  283  284  286  287  288  289  290  291  292\n",
      "  293  295  296  297  298  299  300  301  302  303  304  305  306  308\n",
      "  310  311  312  313  314  315  316  317  318  321  323  324  325  326\n",
      "  327  328  329  330  331  333  334  335  336  337  338  339  340  341\n",
      "  343  344  345  346  347  348  352  353  355  356  357  358  359  360\n",
      "  361  362  363  364  366  367  368  369  370  371  372  373  375  376\n",
      "  379  380  381  382  384  385  386  387  388  389  390  391  392  393\n",
      "  394  395  397  399  401  402  403  404  405  407  408  409  410  411\n",
      "  412  413  414  415  416  417  418  419  420  421  422  423  425  426\n",
      "  427  428  429  431  432  433  436  437  438  439  440  441  442  443\n",
      "  444  445  446  447  448  449  450  451  452  453  454  455  456  457\n",
      "  458  459  460  461  462  464  466  467  468  469  470  471  472  474\n",
      "  476  479  480  481  482  484  485  486  487  489  490  491  492  493\n",
      "  494  495  496  498  499  500  501  502  504  505  506  507  508  509\n",
      "  510  511  512  513  514  515  516  517  519  520  521  522  523  524\n",
      "  525  526  527  528  529  530  531  533  535  537  538  539  540  542\n",
      "  543  545  546  547  550  551  552  553  554  555  556  558  559  561\n",
      "  562  564  565  566  567  570  571  572  573  574  575  576  577  578\n",
      "  580  581  582  583  586  587  588  589  590  591  592  594  595  596\n",
      "  597  598  599  600  601  603  604  606  607  608  609  610  611  612\n",
      "  615  616  617  618  619  620  622  623  624  626  627  630  631  632\n",
      "  633  634  635  636  637  638  639  640  641  642  643  644  645  646\n",
      "  648  649  650  652  653  657  661  663  664  665  666  668  669  670\n",
      "  673  676  677  678  679  680  681  682  685  686  687  688  689  690\n",
      "  691  693  694  695  696  697  698  699  700  701  702  703  704  705\n",
      "  706  708  709  710  711  712  713  714  715  716  717  718  719  720\n",
      "  721  722  723  724  725  726  727  729  730  731  732  733  734  735\n",
      "  736  740  741  742  743  744  745  747  748  749  750  751  753  755\n",
      "  756  757  758  759  760  761  763  764  765  766  767  768  769  770\n",
      "  771  773  775  776  777  778  779  780  781  782  785  786  787  788\n",
      "  789  790  791  792  795  796  797  799  800  801  802  803  804  805\n",
      "  807  808  810  811  812  813  814  815  817  818  820  821  822  823\n",
      "  825  826  827  828  829  831  833  834  835  836  837  838  839  840\n",
      "  841  842  846  847  848  851  852  853  855  856  858  860  862  863\n",
      "  867  868  869  870  873  875  876  877  878  880  882  883  884  885\n",
      "  886  887  888  889  890  891  892  895  896  897  901  902  903  904\n",
      "  905  906  907  908  909  910  911  912  913  915  916  917  918  919\n",
      "  920  921  922  923  925  927  928  930  931  932  933  934  935  936\n",
      "  938  941  942  943  944  945  946  948  949  950  953  954  955  957\n",
      "  959  960  962  963  965  967  968  969  971  973  974  975  978  979\n",
      "  980  981  982  983  984  985  987  988  990  992  993  994  995  996\n",
      "  998  999 1000 1001 1002 1004 1005 1007 1008 1009 1010 1011 1012 1013\n",
      " 1015 1016 1017 1018 1020 1021 1022 1023 1024 1025 1026 1027 1029 1031\n",
      " 1032 1033 1034 1036 1038 1039 1041 1042 1044 1045 1046 1047 1048 1049\n",
      " 1050 1053 1055 1056 1057 1058 1059 1060 1061 1063 1064 1065 1067 1068\n",
      " 1069 1070 1071 1072 1073 1075 1076 1077 1079 1080 1081 1082 1084 1085\n",
      " 1086 1087 1088 1089 1090 1092 1093 1097 1098 1099 1100 1101 1102 1103\n",
      " 1104 1105 1106 1107 1109 1110 1111 1112 1113 1114 1115 1116 1118 1119\n",
      " 1120 1121 1123 1124 1125 1127 1128 1129 1130 1132 1133 1135 1137 1138\n",
      " 1139 1140 1141 1142 1143 1144 1145 1147 1148 1149 1150 1151 1152 1153\n",
      " 1156 1157 1158 1159 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170\n",
      " 1173 1174 1175 1176 1177 1178 1182 1183 1184 1185 1186 1187 1189 1191\n",
      " 1192 1193 1194 1196 1197 1198 1199 1200 1201 1202 1203 1205 1207 1208\n",
      " 1209 1210 1211] Test : [   4   11   18   19   24   28   29   32   33   34   38   49   51   55\n",
      "   59   61   62   83  102  107  110  113  114  116  122  125  128  135\n",
      "  139  146  147  153  155  156  165  168  169  173  176  199  208  209\n",
      "  214  216  223  225  230  231  235  248  249  254  264  276  285  294\n",
      "  307  309  319  320  322  332  342  349  350  351  354  365  374  377\n",
      "  378  383  396  398  400  406  424  430  434  435  463  465  473  475\n",
      "  477  478  483  488  497  503  518  532  534  536  541  544  548  549\n",
      "  557  560  563  568  569  579  584  585  593  602  605  613  614  621\n",
      "  625  628  629  647  651  654  655  656  658  659  660  662  667  671\n",
      "  672  674  675  683  684  692  707  728  737  738  739  746  752  754\n",
      "  762  772  774  783  784  793  794  798  806  809  816  819  824  830\n",
      "  832  843  844  845  849  850  854  857  859  861  864  865  866  871\n",
      "  872  874  879  881  893  894  898  899  900  914  924  926  929  937\n",
      "  939  940  947  951  952  956  958  961  964  966  970  972  976  977\n",
      "  986  989  991  997 1003 1006 1014 1019 1028 1030 1035 1037 1040 1043\n",
      " 1051 1052 1054 1062 1066 1074 1078 1083 1091 1094 1095 1096 1108 1117\n",
      " 1122 1126 1131 1134 1136 1146 1154 1155 1160 1171 1172 1179 1180 1181\n",
      " 1188 1190 1195 1204 1206]\n",
      "Train : [   0    1    3    4    5    8    9   10   11   12   13   14   16   18\n",
      "   19   20   22   23   24   27   28   29   32   33   34   35   36   37\n",
      "   38   39   41   42   44   45   46   47   49   50   51   52   54   55\n",
      "   56   57   58   59   61   62   63   64   65   67   70   72   75   77\n",
      "   78   79   81   82   83   84   85   86   87   88   90   93   94   96\n",
      "   97   99  100  101  102  104  105  106  107  108  109  110  111  112\n",
      "  113  114  116  117  118  121  122  124  125  126  127  128  129  131\n",
      "  132  134  135  136  138  139  140  144  146  147  148  149  150  151\n",
      "  152  153  154  155  156  157  158  159  160  161  162  164  165  166\n",
      "  167  168  169  170  172  173  174  175  176  178  179  181  182  183\n",
      "  184  185  186  187  188  189  190  191  192  196  197  199  200  201\n",
      "  202  203  204  205  206  207  208  209  210  211  212  214  215  216\n",
      "  217  218  219  220  221  222  223  224  225  226  227  228  229  230\n",
      "  231  232  233  234  235  236  238  239  240  241  244  245  246  247\n",
      "  248  249  250  251  252  253  254  255  256  257  258  259  260  261\n",
      "  262  263  264  265  266  267  268  269  270  271  272  274  275  276\n",
      "  277  278  279  280  283  284  285  286  287  288  290  293  294  296\n",
      "  298  300  302  303  304  306  307  308  309  310  311  314  315  316\n",
      "  318  319  320  321  322  327  328  329  330  332  334  335  336  337\n",
      "  338  340  342  344  345  346  347  348  349  350  351  352  354  355\n",
      "  356  359  360  361  362  364  365  366  367  368  369  370  371  372\n",
      "  373  374  375  376  377  378  380  381  382  383  384  385  388  389\n",
      "  390  393  394  395  396  397  398  399  400  401  402  403  404  405\n",
      "  406  407  408  409  410  411  412  413  414  415  416  417  420  421\n",
      "  422  423  424  425  426  429  430  431  432  433  434  435  436  437\n",
      "  438  440  442  443  444  445  447  448  450  451  452  453  454  457\n",
      "  458  461  462  463  465  466  467  468  469  470  471  472  473  474\n",
      "  475  476  477  478  479  480  481  482  483  485  486  487  488  489\n",
      "  492  493  495  497  498  500  501  502  503  504  505  506  511  512\n",
      "  513  515  516  518  519  521  522  523  524  525  526  527  528  529\n",
      "  531  532  533  534  536  537  538  541  542  543  544  545  546  547\n",
      "  548  549  551  552  553  554  555  556  557  560  561  562  563  564\n",
      "  568  569  570  572  573  574  575  576  579  580  582  583  584  585\n",
      "  586  587  589  591  592  593  594  595  596  598  599  600  601  602\n",
      "  603  605  606  607  609  610  611  613  614  616  618  619  620  621\n",
      "  622  623  625  626  628  629  630  632  634  635  636  639  640  643\n",
      "  645  647  650  651  652  653  654  655  656  657  658  659  660  661\n",
      "  662  664  665  667  670  671  672  673  674  675  676  677  678  679\n",
      "  680  682  683  684  686  687  688  689  690  692  693  694  695  696\n",
      "  697  698  699  700  701  702  703  704  705  706  707  708  709  710\n",
      "  711  712  713  714  715  716  717  719  720  721  722  724  725  726\n",
      "  727  728  729  730  731  732  733  734  736  737  738  739  740  741\n",
      "  743  744  745  746  747  749  750  752  753  754  757  758  759  760\n",
      "  761  762  763  764  765  767  771  772  773  774  775  777  778  779\n",
      "  780  781  782  783  784  785  786  787  788  789  790  791  792  793\n",
      "  794  795  798  800  801  802  803  804  806  807  808  809  810  811\n",
      "  812  814  815  816  817  819  820  822  823  824  825  827  828  829\n",
      "  830  832  833  834  836  837  838  839  840  841  843  844  845  846\n",
      "  847  848  849  850  851  852  853  854  855  856  857  858  859  860\n",
      "  861  862  863  864  865  866  867  870  871  872  873  874  875  878\n",
      "  879  880  881  882  883  884  886  888  889  890  893  894  895  896\n",
      "  897  898  899  900  901  902  904  905  906  907  908  909  911  912\n",
      "  913  914  915  916  917  918  919  920  921  922  923  924  926  927\n",
      "  929  930  931  932  933  934  935  936  937  938  939  940  941  943\n",
      "  944  945  946  947  948  949  950  951  952  953  954  956  957  958\n",
      "  960  961  962  963  964  965  966  967  970  972  973  974  975  976\n",
      "  977  978  979  981  982  983  985  986  988  989  991  992  994  996\n",
      "  997  998  999 1001 1003 1004 1006 1007 1008 1010 1011 1012 1013 1014\n",
      " 1016 1017 1018 1019 1020 1021 1024 1025 1026 1028 1029 1030 1031 1032\n",
      " 1035 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1048 1049 1050\n",
      " 1051 1052 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1066\n",
      " 1067 1068 1069 1070 1073 1074 1075 1076 1077 1078 1080 1081 1082 1083\n",
      " 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1098 1100 1102\n",
      " 1103 1104 1105 1106 1108 1110 1112 1113 1114 1115 1116 1117 1120 1121\n",
      " 1122 1124 1125 1126 1128 1129 1130 1131 1132 1134 1135 1136 1138 1139\n",
      " 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153\n",
      " 1154 1155 1158 1159 1160 1162 1163 1164 1166 1168 1169 1170 1171 1172\n",
      " 1173 1174 1177 1178 1179 1180 1181 1182 1183 1184 1186 1187 1188 1189\n",
      " 1190 1192 1193 1194 1195 1196 1197 1200 1201 1204 1205 1206 1207 1208\n",
      " 1209 1210 1211] Test : [   2    6    7   15   17   21   25   26   30   31   40   43   48   53\n",
      "   60   66   68   69   71   73   74   76   80   89   91   92   95   98\n",
      "  103  115  119  120  123  130  133  137  141  142  143  145  163  171\n",
      "  177  180  193  194  195  198  213  237  242  243  273  281  282  289\n",
      "  291  292  295  297  299  301  305  312  313  317  323  324  325  326\n",
      "  331  333  339  341  343  353  357  358  363  379  386  387  391  392\n",
      "  418  419  427  428  439  441  446  449  455  456  459  460  464  484\n",
      "  490  491  494  496  499  507  508  509  510  514  517  520  530  535\n",
      "  539  540  550  558  559  565  566  567  571  577  578  581  588  590\n",
      "  597  604  608  612  615  617  624  627  631  633  637  638  641  642\n",
      "  644  646  648  649  663  666  668  669  681  685  691  718  723  735\n",
      "  742  748  751  755  756  766  768  769  770  776  796  797  799  805\n",
      "  813  818  821  826  831  835  842  868  869  876  877  885  887  891\n",
      "  892  903  910  925  928  942  955  959  968  969  971  980  984  987\n",
      "  990  993  995 1000 1002 1005 1009 1015 1022 1023 1027 1033 1034 1036\n",
      " 1047 1053 1065 1071 1072 1079 1084 1085 1097 1099 1101 1107 1109 1111\n",
      " 1118 1119 1123 1127 1133 1137 1156 1157 1161 1165 1167 1175 1176 1185\n",
      " 1191 1198 1199 1202 1203]\n",
      "Train : [   1    2    4    6    7    8    9   11   13   15   16   17   18   19\n",
      "   20   21   22   23   24   25   26   27   28   29   30   31   32   33\n",
      "   34   35   36   38   40   42   43   45   47   48   49   51   52   53\n",
      "   54   55   56   58   59   60   61   62   63   64   65   66   67   68\n",
      "   69   70   71   73   74   75   76   79   80   82   83   84   85   87\n",
      "   89   91   92   95   97   98   99  100  102  103  105  106  107  109\n",
      "  110  111  112  113  114  115  116  118  119  120  122  123  124  125\n",
      "  126  127  128  130  131  132  133  134  135  137  138  139  140  141\n",
      "  142  143  145  146  147  148  149  152  153  154  155  156  157  158\n",
      "  162  163  165  167  168  169  170  171  172  173  174  176  177  180\n",
      "  182  184  185  186  189  191  193  194  195  197  198  199  201  202\n",
      "  203  204  207  208  209  211  213  214  215  216  218  220  221  222\n",
      "  223  225  226  227  228  229  230  231  232  233  234  235  236  237\n",
      "  238  239  240  241  242  243  244  245  246  247  248  249  250  251\n",
      "  252  253  254  257  258  259  261  262  264  265  267  268  269  270\n",
      "  271  272  273  275  276  277  278  279  281  282  283  285  286  287\n",
      "  289  290  291  292  294  295  296  297  298  299  300  301  302  303\n",
      "  304  305  307  308  309  310  311  312  313  315  316  317  318  319\n",
      "  320  321  322  323  324  325  326  327  328  329  330  331  332  333\n",
      "  334  335  337  339  340  341  342  343  344  346  347  349  350  351\n",
      "  352  353  354  355  357  358  359  360  361  362  363  364  365  367\n",
      "  368  370  371  373  374  375  376  377  378  379  380  381  383  384\n",
      "  385  386  387  388  391  392  393  394  395  396  398  399  400  401\n",
      "  406  407  408  412  413  415  416  418  419  420  421  423  424  425\n",
      "  426  427  428  429  430  433  434  435  437  438  439  441  443  444\n",
      "  445  446  447  449  450  451  452  453  454  455  456  459  460  461\n",
      "  462  463  464  465  466  467  468  471  473  474  475  477  478  480\n",
      "  481  482  483  484  486  488  489  490  491  492  493  494  496  497\n",
      "  498  499  502  503  505  507  508  509  510  511  514  515  516  517\n",
      "  518  519  520  522  523  524  526  527  528  529  530  531  532  533\n",
      "  534  535  536  537  538  539  540  541  544  546  547  548  549  550\n",
      "  551  552  555  557  558  559  560  563  564  565  566  567  568  569\n",
      "  571  572  573  574  577  578  579  581  582  583  584  585  587  588\n",
      "  589  590  592  593  596  597  598  599  600  601  602  603  604  605\n",
      "  607  608  610  612  613  614  615  617  619  620  621  623  624  625\n",
      "  627  628  629  630  631  632  633  634  635  637  638  639  640  641\n",
      "  642  644  645  646  647  648  649  650  651  652  653  654  655  656\n",
      "  657  658  659  660  661  662  663  664  665  666  667  668  669  670\n",
      "  671  672  674  675  676  677  678  680  681  682  683  684  685  687\n",
      "  689  691  692  695  696  699  701  702  703  705  706  707  708  709\n",
      "  710  711  712  713  715  716  717  718  720  721  722  723  724  725\n",
      "  727  728  729  730  731  732  733  734  735  736  737  738  739  740\n",
      "  741  742  744  746  748  751  752  753  754  755  756  758  759  761\n",
      "  762  765  766  767  768  769  770  772  773  774  776  777  778  779\n",
      "  782  783  784  785  786  788  789  791  793  794  795  796  797  798\n",
      "  799  800  802  805  806  807  808  809  810  813  815  816  818  819\n",
      "  821  822  824  825  826  827  828  829  830  831  832  834  835  836\n",
      "  837  838  839  840  841  842  843  844  845  846  847  849  850  852\n",
      "  853  854  855  857  858  859  860  861  863  864  865  866  868  869\n",
      "  871  872  874  876  877  878  879  880  881  883  884  885  887  888\n",
      "  889  891  892  893  894  895  897  898  899  900  901  903  904  905\n",
      "  906  907  908  910  914  916  917  918  919  920  921  922  923  924\n",
      "  925  926  927  928  929  930  932  933  934  937  938  939  940  941\n",
      "  942  943  944  946  947  948  949  951  952  954  955  956  957  958\n",
      "  959  960  961  964  966  967  968  969  970  971  972  974  975  976\n",
      "  977  978  980  981  982  983  984  985  986  987  988  989  990  991\n",
      "  992  993  995  997  998  999 1000 1001 1002 1003 1004 1005 1006 1008\n",
      " 1009 1010 1011 1013 1014 1015 1018 1019 1020 1021 1022 1023 1024 1025\n",
      " 1026 1027 1028 1029 1030 1032 1033 1034 1035 1036 1037 1038 1039 1040\n",
      " 1041 1043 1044 1045 1046 1047 1048 1051 1052 1053 1054 1056 1058 1059\n",
      " 1060 1061 1062 1063 1065 1066 1068 1070 1071 1072 1073 1074 1075 1077\n",
      " 1078 1079 1081 1083 1084 1085 1086 1087 1088 1090 1091 1092 1093 1094\n",
      " 1095 1096 1097 1098 1099 1100 1101 1102 1104 1105 1107 1108 1109 1110\n",
      " 1111 1112 1113 1116 1117 1118 1119 1120 1122 1123 1124 1125 1126 1127\n",
      " 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141\n",
      " 1142 1143 1144 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156\n",
      " 1157 1159 1160 1161 1162 1163 1164 1165 1166 1167 1169 1170 1171 1172\n",
      " 1173 1174 1175 1176 1177 1178 1179 1180 1181 1183 1184 1185 1186 1187\n",
      " 1188 1189 1190 1191 1192 1193 1194 1195 1197 1198 1199 1201 1202 1203\n",
      " 1204 1206 1207 1209] Test : [   0    3    5   10   12   14   37   39   41   44   46   50   57   72\n",
      "   77   78   81   86   88   90   93   94   96  101  104  108  117  121\n",
      "  129  136  144  150  151  159  160  161  164  166  175  178  179  181\n",
      "  183  187  188  190  192  196  200  205  206  210  212  217  219  224\n",
      "  255  256  260  263  266  274  280  284  288  293  306  314  336  338\n",
      "  345  348  356  366  369  372  382  389  390  397  402  403  404  405\n",
      "  409  410  411  414  417  422  431  432  436  440  442  448  457  458\n",
      "  469  470  472  476  479  485  487  495  500  501  504  506  512  513\n",
      "  521  525  542  543  545  553  554  556  561  562  570  575  576  580\n",
      "  586  591  594  595  606  609  611  616  618  622  626  636  643  673\n",
      "  679  686  688  690  693  694  697  698  700  704  714  719  726  743\n",
      "  745  747  749  750  757  760  763  764  771  775  780  781  787  790\n",
      "  792  801  803  804  811  812  814  817  820  823  833  848  851  856\n",
      "  862  867  870  873  875  882  886  890  896  902  909  911  912  913\n",
      "  915  931  935  936  945  950  953  962  963  965  973  979  994  996\n",
      " 1007 1012 1016 1017 1031 1042 1049 1050 1055 1057 1064 1067 1069 1076\n",
      " 1080 1082 1089 1103 1106 1114 1115 1121 1145 1158 1168 1182 1196 1200\n",
      " 1205 1208 1210 1211]\n",
      "Train : [   0    1    2    3    4    5    6    7    8    9   10   11   12   14\n",
      "   15   16   17   18   19   20   21   24   25   26   27   28   29   30\n",
      "   31   32   33   34   35   36   37   38   39   40   41   43   44   46\n",
      "   48   49   50   51   52   53   54   55   57   59   60   61   62   63\n",
      "   65   66   68   69   71   72   73   74   75   76   77   78   80   81\n",
      "   82   83   86   88   89   90   91   92   93   94   95   96   97   98\n",
      "  101  102  103  104  107  108  109  110  111  112  113  114  115  116\n",
      "  117  119  120  121  122  123  124  125  128  129  130  131  132  133\n",
      "  135  136  137  138  139  140  141  142  143  144  145  146  147  149\n",
      "  150  151  152  153  154  155  156  157  158  159  160  161  163  164\n",
      "  165  166  167  168  169  171  172  173  174  175  176  177  178  179\n",
      "  180  181  183  185  186  187  188  190  192  193  194  195  196  197\n",
      "  198  199  200  201  202  205  206  207  208  209  210  212  213  214\n",
      "  216  217  219  223  224  225  226  229  230  231  234  235  237  238\n",
      "  240  241  242  243  245  246  247  248  249  250  251  254  255  256\n",
      "  258  260  263  264  265  266  271  273  274  275  276  277  278  279\n",
      "  280  281  282  283  284  285  286  288  289  290  291  292  293  294\n",
      "  295  296  297  298  299  301  302  305  306  307  309  310  312  313\n",
      "  314  315  316  317  319  320  322  323  324  325  326  327  329  330\n",
      "  331  332  333  336  337  338  339  340  341  342  343  344  345  346\n",
      "  347  348  349  350  351  352  353  354  356  357  358  361  363  365\n",
      "  366  367  368  369  372  373  374  377  378  379  382  383  384  386\n",
      "  387  389  390  391  392  395  396  397  398  399  400  401  402  403\n",
      "  404  405  406  407  409  410  411  414  415  416  417  418  419  422\n",
      "  424  426  427  428  429  430  431  432  433  434  435  436  437  439\n",
      "  440  441  442  444  446  448  449  450  453  455  456  457  458  459\n",
      "  460  462  463  464  465  467  468  469  470  472  473  474  475  476\n",
      "  477  478  479  481  482  483  484  485  486  487  488  490  491  494\n",
      "  495  496  497  499  500  501  503  504  506  507  508  509  510  511\n",
      "  512  513  514  515  517  518  519  520  521  522  523  525  527  528\n",
      "  530  532  533  534  535  536  537  539  540  541  542  543  544  545\n",
      "  547  548  549  550  551  552  553  554  555  556  557  558  559  560\n",
      "  561  562  563  565  566  567  568  569  570  571  572  574  575  576\n",
      "  577  578  579  580  581  582  584  585  586  588  589  590  591  592\n",
      "  593  594  595  596  597  599  602  603  604  605  606  607  608  609\n",
      "  610  611  612  613  614  615  616  617  618  619  621  622  624  625\n",
      "  626  627  628  629  631  633  635  636  637  638  639  640  641  642\n",
      "  643  644  645  646  647  648  649  650  651  654  655  656  657  658\n",
      "  659  660  662  663  665  666  667  668  669  671  672  673  674  675\n",
      "  677  678  679  681  683  684  685  686  688  689  690  691  692  693\n",
      "  694  695  696  697  698  700  702  704  707  708  709  710  714  718\n",
      "  719  721  723  724  725  726  728  729  731  733  734  735  737  738\n",
      "  739  742  743  744  745  746  747  748  749  750  751  752  754  755\n",
      "  756  757  760  761  762  763  764  765  766  767  768  769  770  771\n",
      "  772  773  774  775  776  777  778  779  780  781  782  783  784  785\n",
      "  787  789  790  791  792  793  794  795  796  797  798  799  801  803\n",
      "  804  805  806  809  810  811  812  813  814  815  816  817  818  819\n",
      "  820  821  823  824  826  828  830  831  832  833  835  836  840  841\n",
      "  842  843  844  845  846  848  849  850  851  853  854  855  856  857\n",
      "  858  859  861  862  863  864  865  866  867  868  869  870  871  872\n",
      "  873  874  875  876  877  879  881  882  883  884  885  886  887  888\n",
      "  890  891  892  893  894  895  896  897  898  899  900  902  903  905\n",
      "  907  908  909  910  911  912  913  914  915  916  918  922  924  925\n",
      "  926  928  929  930  931  934  935  936  937  938  939  940  941  942\n",
      "  944  945  946  947  950  951  952  953  955  956  958  959  961  962\n",
      "  963  964  965  966  967  968  969  970  971  972  973  974  976  977\n",
      "  978  979  980  984  986  987  988  989  990  991  992  993  994  995\n",
      "  996  997 1000 1001 1002 1003 1005 1006 1007 1009 1010 1011 1012 1013\n",
      " 1014 1015 1016 1017 1019 1020 1021 1022 1023 1025 1027 1028 1029 1030\n",
      " 1031 1032 1033 1034 1035 1036 1037 1039 1040 1042 1043 1045 1047 1049\n",
      " 1050 1051 1052 1053 1054 1055 1056 1057 1060 1061 1062 1064 1065 1066\n",
      " 1067 1068 1069 1070 1071 1072 1074 1076 1077 1078 1079 1080 1081 1082\n",
      " 1083 1084 1085 1089 1090 1091 1092 1094 1095 1096 1097 1099 1101 1102\n",
      " 1103 1104 1106 1107 1108 1109 1110 1111 1113 1114 1115 1117 1118 1119\n",
      " 1121 1122 1123 1124 1126 1127 1130 1131 1133 1134 1135 1136 1137 1138\n",
      " 1139 1141 1143 1145 1146 1148 1149 1150 1151 1153 1154 1155 1156 1157\n",
      " 1158 1159 1160 1161 1163 1165 1166 1167 1168 1169 1171 1172 1173 1174\n",
      " 1175 1176 1177 1179 1180 1181 1182 1184 1185 1187 1188 1190 1191 1192\n",
      " 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207\n",
      " 1208 1209 1210 1211] Test : [  13   22   23   42   45   47   56   58   64   67   70   79   84   85\n",
      "   87   99  100  105  106  118  126  127  134  148  162  170  182  184\n",
      "  189  191  203  204  211  215  218  220  221  222  227  228  232  233\n",
      "  236  239  244  252  253  257  259  261  262  267  268  269  270  272\n",
      "  287  300  303  304  308  311  318  321  328  334  335  355  359  360\n",
      "  362  364  370  371  375  376  380  381  385  388  393  394  408  412\n",
      "  413  420  421  423  425  438  443  445  447  451  452  454  461  466\n",
      "  471  480  489  492  493  498  502  505  516  524  526  529  531  538\n",
      "  546  564  573  583  587  598  600  601  620  623  630  632  634  652\n",
      "  653  661  664  670  676  680  682  687  699  701  703  705  706  711\n",
      "  712  713  715  716  717  720  722  727  730  732  736  740  741  753\n",
      "  758  759  786  788  800  802  807  808  822  825  827  829  834  837\n",
      "  838  839  847  852  860  878  880  889  901  904  906  917  919  920\n",
      "  921  923  927  932  933  943  948  949  954  957  960  975  981  982\n",
      "  983  985  998  999 1004 1008 1018 1024 1026 1038 1041 1044 1046 1048\n",
      " 1058 1059 1063 1073 1075 1086 1087 1088 1093 1098 1100 1105 1112 1116\n",
      " 1120 1125 1128 1129 1132 1140 1142 1144 1147 1152 1162 1164 1170 1178\n",
      " 1183 1186 1189 1193]\n",
      "Train : [   0    2    3    4    5    6    7   10   11   12   13   14   15   17\n",
      "   18   19   21   22   23   24   25   26   28   29   30   31   32   33\n",
      "   34   37   38   39   40   41   42   43   44   45   46   47   48   49\n",
      "   50   51   53   55   56   57   58   59   60   61   62   64   66   67\n",
      "   68   69   70   71   72   73   74   76   77   78   79   80   81   83\n",
      "   84   85   86   87   88   89   90   91   92   93   94   95   96   98\n",
      "   99  100  101  102  103  104  105  106  107  108  110  113  114  115\n",
      "  116  117  118  119  120  121  122  123  125  126  127  128  129  130\n",
      "  133  134  135  136  137  139  141  142  143  144  145  146  147  148\n",
      "  150  151  153  155  156  159  160  161  162  163  164  165  166  168\n",
      "  169  170  171  173  175  176  177  178  179  180  181  182  183  184\n",
      "  187  188  189  190  191  192  193  194  195  196  198  199  200  203\n",
      "  204  205  206  208  209  210  211  212  213  214  215  216  217  218\n",
      "  219  220  221  222  223  224  225  227  228  230  231  232  233  235\n",
      "  236  237  239  242  243  244  248  249  252  253  254  255  256  257\n",
      "  259  260  261  262  263  264  266  267  268  269  270  272  273  274\n",
      "  276  280  281  282  284  285  287  288  289  291  292  293  294  295\n",
      "  297  299  300  301  303  304  305  306  307  308  309  311  312  313\n",
      "  314  317  318  319  320  321  322  323  324  325  326  328  331  332\n",
      "  333  334  335  336  338  339  341  342  343  345  348  349  350  351\n",
      "  353  354  355  356  357  358  359  360  362  363  364  365  366  369\n",
      "  370  371  372  374  375  376  377  378  379  380  381  382  383  385\n",
      "  386  387  388  389  390  391  392  393  394  396  397  398  400  402\n",
      "  403  404  405  406  408  409  410  411  412  413  414  417  418  419\n",
      "  420  421  422  423  424  425  427  428  430  431  432  434  435  436\n",
      "  438  439  440  441  442  443  445  446  447  448  449  451  452  454\n",
      "  455  456  457  458  459  460  461  463  464  465  466  469  470  471\n",
      "  472  473  475  476  477  478  479  480  483  484  485  487  488  489\n",
      "  490  491  492  493  494  495  496  497  498  499  500  501  502  503\n",
      "  504  505  506  507  508  509  510  512  513  514  516  517  518  520\n",
      "  521  524  525  526  529  530  531  532  534  535  536  538  539  540\n",
      "  541  542  543  544  545  546  548  549  550  553  554  556  557  558\n",
      "  559  560  561  562  563  564  565  566  567  568  569  570  571  573\n",
      "  575  576  577  578  579  580  581  583  584  585  586  587  588  590\n",
      "  591  593  594  595  597  598  600  601  602  604  605  606  608  609\n",
      "  611  612  613  614  615  616  617  618  620  621  622  623  624  625\n",
      "  626  627  628  629  630  631  632  633  634  636  637  638  641  642\n",
      "  643  644  646  647  648  649  651  652  653  654  655  656  658  659\n",
      "  660  661  662  663  664  666  667  668  669  670  671  672  673  674\n",
      "  675  676  679  680  681  682  683  684  685  686  687  688  690  691\n",
      "  692  693  694  697  698  699  700  701  703  704  705  706  707  711\n",
      "  712  713  714  715  716  717  718  719  720  722  723  726  727  728\n",
      "  730  732  735  736  737  738  739  740  741  742  743  745  746  747\n",
      "  748  749  750  751  752  753  754  755  756  757  758  759  760  762\n",
      "  763  764  766  768  769  770  771  772  774  775  776  780  781  783\n",
      "  784  786  787  788  790  792  793  794  796  797  798  799  800  801\n",
      "  802  803  804  805  806  807  808  809  811  812  813  814  816  817\n",
      "  818  819  820  821  822  823  824  825  826  827  829  830  831  832\n",
      "  833  834  835  837  838  839  842  843  844  845  847  848  849  850\n",
      "  851  852  854  856  857  859  860  861  862  864  865  866  867  868\n",
      "  869  870  871  872  873  874  875  876  877  878  879  880  881  882\n",
      "  885  886  887  889  890  891  892  893  894  896  898  899  900  901\n",
      "  902  903  904  906  909  910  911  912  913  914  915  917  919  920\n",
      "  921  923  924  925  926  927  928  929  931  932  933  935  936  937\n",
      "  939  940  942  943  945  947  948  949  950  951  952  953  954  955\n",
      "  956  957  958  959  960  961  962  963  964  965  966  968  969  970\n",
      "  971  972  973  975  976  977  979  980  981  982  983  984  985  986\n",
      "  987  989  990  991  993  994  995  996  997  998  999 1000 1002 1003\n",
      " 1004 1005 1006 1007 1008 1009 1012 1014 1015 1016 1017 1018 1019 1022\n",
      " 1023 1024 1026 1027 1028 1030 1031 1033 1034 1035 1036 1037 1038 1040\n",
      " 1041 1042 1043 1044 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055\n",
      " 1057 1058 1059 1062 1063 1064 1065 1066 1067 1069 1071 1072 1073 1074\n",
      " 1075 1076 1078 1079 1080 1082 1083 1084 1085 1086 1087 1088 1089 1091\n",
      " 1093 1094 1095 1096 1097 1098 1099 1100 1101 1103 1105 1106 1107 1108\n",
      " 1109 1111 1112 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1125\n",
      " 1126 1127 1128 1129 1131 1132 1133 1134 1136 1137 1140 1142 1144 1145\n",
      " 1146 1147 1152 1154 1155 1156 1157 1158 1160 1161 1162 1164 1165 1167\n",
      " 1168 1170 1171 1172 1175 1176 1178 1179 1180 1181 1182 1183 1185 1186\n",
      " 1188 1189 1190 1191 1193 1195 1196 1198 1199 1200 1202 1203 1204 1205\n",
      " 1206 1208 1210 1211] Test : [   1    8    9   16   20   27   35   36   52   54   63   65   75   82\n",
      "   97  109  111  112  124  131  132  138  140  149  152  154  157  158\n",
      "  167  172  174  185  186  197  201  202  207  226  229  234  238  240\n",
      "  241  245  246  247  250  251  258  265  271  275  277  278  279  283\n",
      "  286  290  296  298  302  310  315  316  327  329  330  337  340  344\n",
      "  346  347  352  361  367  368  373  384  395  399  401  407  415  416\n",
      "  426  429  433  437  444  450  453  462  467  468  474  481  482  486\n",
      "  511  515  519  522  523  527  528  533  537  547  551  552  555  572\n",
      "  574  582  589  592  596  599  603  607  610  619  635  639  640  645\n",
      "  650  657  665  677  678  689  695  696  702  708  709  710  721  724\n",
      "  725  729  731  733  734  744  761  765  767  773  777  778  779  782\n",
      "  785  789  791  795  810  815  828  836  840  841  846  853  855  858\n",
      "  863  883  884  888  895  897  905  907  908  916  918  922  930  934\n",
      "  938  941  944  946  967  974  978  988  992 1001 1010 1011 1013 1020\n",
      " 1021 1025 1029 1032 1039 1045 1056 1060 1061 1068 1070 1077 1081 1090\n",
      " 1092 1102 1104 1110 1113 1124 1130 1135 1138 1139 1141 1143 1148 1149\n",
      " 1150 1151 1153 1159 1163 1166 1169 1173 1174 1177 1184 1187 1192 1194\n",
      " 1197 1201 1207 1209]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skfold2.split(X, y) : \n",
    "    print(\"Train :\", train_index, \"Test :\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.values.reshape(-1,1)[train_index], y.values.reshape(-1,1)[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.05, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=4, max_leaves=0, min_child_weight=2, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=200, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.05, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=4, max_leaves=0, min_child_weight=2, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=200, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.05, max_bin=256,\n",
       "              max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "              max_depth=4, max_leaves=0, min_child_weight=2, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=200, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(learning_rate=0.05, gamma=0, max_depth=4, min_child_weight=2, n_estimators=200)\n",
    "xgb2.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xgb2.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9132231404958677\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb2.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "pred_test = [7.30,19.80,-1.70,53.9,390.280,14.1,28.00,3.80,38.50,496.970,17.7,28.50,8.10,97.70,560.500]\n",
    "pred_test = np.array(pred_test).reshape(1,-1)\n",
    "pred_test = std.transform(pred_test)\n",
    "\n",
    "y_pred2 = xgb2.predict(pred_test)\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "pred_test2 = [5.25,19.75,-8.6,2.55,455.28,10.85,23.,-2.2,23.55,530.950,17.05,31.4,5.55,67.55,562.770]\n",
    "pred_test2 = np.array(pred_test2).reshape(1,-1)\n",
    "pred_test2 = std.transform(pred_test2)\n",
    "\n",
    "y_pred3 = xgb2.predict(pred_test2)\n",
    "print(y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "pred_test3 = [6.60,21.30,-6.0,28.50,523.00,11.90,25.9,-1.4,49.00,540.765,17.50,34.5,5.60,52.60,586.195]\n",
    "pred_test3 = np.array(pred_test3).reshape(1,-1)\n",
    "pred_test3 = std.transform(pred_test3)\n",
    "\n",
    "y_pred4= xgb2.predict(pred_test3)\n",
    "print(y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score : 0.9804123711340206\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = xgb2.predict(X_train)\n",
    "print(\"accuracy_score :\", accuracy_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "pred_test4 = [25.3,\t34.00,\t18.90,\t358.70,\t496.760,\t22.10,\t30.2,\t13.50,\t139.90,\t415.265,\t15.60,\t30.60,\t0.90,\t43.20,\t423.275]\n",
    "pred_test4 = np.array(pred_test4).reshape(1,-1)\n",
    "pred_test4 = std.transform(pred_test4)\n",
    "\n",
    "y_pred5= xgb2.predict(pred_test4)\n",
    "print(y_pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 적합 판단 모델 저장\n",
    "f = open(\"xgb_baechoo_bin_classify_jinhyeok.pickle\", \"wb\")\n",
    "pickle.dump(xgb2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "## 적합 판단 모델 불러오기\n",
    "with open('xgb_baechoo_bin_classify_jinhyeok.pickle', 'rb') as f: \n",
    "    model = pickle.load(f)\n",
    "    y_p = model.predict(pred_test4)\n",
    "    print(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_baechoo_bin_classify_scaler_jinhyeok.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardscaler 저장\n",
    "# o = open(\"xgb_baechoo_bin_classify_scaler_jinhyeok.pickle\", \"wb\")\n",
    "# pickle.dump(std,o)\n",
    "\n",
    "# o = open(\"xgb_baechoo_bin_classify_scaler_jinhyeok.pkl\", \"wb\")\n",
    "joblib.dump(std,\"xgb_baechoo_bin_classify_scaler_jinhyeok.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86837142  0.78583699  1.03906174  1.54897321  0.14175334  0.56780096\n",
      "   0.07846137  0.5301293  -0.2011774  -0.58218877 -0.47677394  0.25288718\n",
      "  -0.91917529 -0.68473015 -0.33459753]]\n"
     ]
    }
   ],
   "source": [
    "pred_test4 = [25.3,\t34.00,\t18.90,\t358.70,\t496.760,\t22.10,\t30.2,\t13.50,\t139.90,\t415.265,\t15.60,\t30.60,\t0.90,\t43.20,\t423.275]\n",
    "pred_test4 = np.array(pred_test4).reshape(1,-1)\n",
    "\n",
    "# with open(\"xgb_baechoo_bin_classify_scaler_jinhyeok.pickle\", \"rb\") as f : \n",
    "#     scaler = pickle.load(f)\n",
    "#     scaling = scaler.transform(pred_test4)\n",
    "#     print(scaling)\n",
    "scaler = joblib.load(\"xgb_baechoo_bin_classify_scaler_jinhyeok.pkl\")\n",
    "scaling = scaler.transform(pred_test4)\n",
    "print(scaling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression을 통한 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.7366255144032922\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8847736625514403\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [15.90,21.80,12.20,133.50,471.260,17.80,25.10,18.40,384.00,529.200,20.7,26.60,19.70,415.00,463.430]\n",
    "test_array = np.array(test).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict(test_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseDeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct 19 2022, 17:52:09) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5bd3b74290b6fecca2d77c6682b8ba7e9275f0a56c500dd407ba5b0bc3fc494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
